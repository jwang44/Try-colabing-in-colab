{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECSE01_2_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/Try-colabing-in-colab/blob/main/orthopedic_patients_new_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0NkwhApnnJk"
      },
      "source": [
        "# ECSE 551 Mini-project 1\n",
        "*Group 10: Junhao Wang, Yinan Zhou, and Ruilin Ji*\n",
        "\n",
        "This notebook is dedicated for the **Orthopedic patients dataset**, including the model, cross validation, and various experiment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3nM6IIOBf0A"
      },
      "source": [
        "## Start here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oskj54Fv8eGB",
        "outputId": "a012f816-2d1c-44da-d811-064e6f3e9e43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrLPT4OTIt5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4f7fe1-6a27-4222-ad4f-c7f47358bf34"
      },
      "source": [
        "%cd \"/content/drive/My Drive/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFoqVsWOP14"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import scipy.stats\r\n",
        "import statistics"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1gREIMPzw7M"
      },
      "source": [
        "## Orthopedic Patients Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-nyMGN0pKx"
      },
      "source": [
        "# generate new feature by multiplication and normalize\r\n",
        "def newfeature(x,y):\r\n",
        "  z=x*y\r\n",
        "  norz=scipy.stats.zscore(z, axis=0, ddof=0, nan_policy='propagate')\r\n",
        "  return norz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc7bhCtyW17b"
      },
      "source": [
        "# convert csv to dataframe\n",
        "df = pd.read_csv('orthopedic_patients.csv')\n",
        "original_data = df.to_numpy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybt1JT3h0JpU"
      },
      "source": [
        "# normalize feature\r\n",
        "NorData = scipy.stats.zscore(df.iloc[:,:-1], axis=0, ddof=0, nan_policy='propagate')\r\n",
        "# normalized dataset\r\n",
        "NorDataset = np.column_stack((NorData, df['Class']))\r\n",
        "# np.savetxt('normalized_orthopedic_patients.csv', NorPatientData, delimiter=',')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oDHjN7u0rbT"
      },
      "source": [
        "New feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drHfVd9v2elN",
        "outputId": "f3540865-56a7-46a4-aff5-8762af8cc987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''# new feature\r\n",
        "NewF1 = newfeature(df.pelvic_incidence, df.sacral_slope)\r\n",
        "n_row = np.shape(original_data)[0]\r\n",
        "NewF = np.zeros(n_row)\r\n",
        "for col in range(6):\r\n",
        "  new = newfeature(df.iloc[:,col],df.iloc[:,col]) # square feature\r\n",
        "  np.concatenate((NewF,new))\r\n",
        "print(NewF)\r\n",
        "# normalized dataset with new feature\r\n",
        "NorDatasetNew = np.column_stack((NorData,NewF1,NewF,df['Class']))\r\n",
        "np.shape(NewF)'''"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0072811  -1.06927487  0.34397016  0.37222718 -0.65312577 -1.02765119\n",
            " -0.48432133 -0.8349147  -0.89676982 -1.14828133 -0.6533882  -1.31124416\n",
            " -0.68770788 -0.47777787 -0.29593917 -0.8762646   0.05232096 -1.31003418\n",
            " -1.08155524 -0.9742737  -0.89163761 -0.41348692  0.00983303 -0.82795581\n",
            " -1.16621956 -0.45157691 -1.43958197 -0.90480327 -0.86719409  0.22736821\n",
            " -0.6042101  -0.79361914 -0.85199769 -1.08274642 -0.1778636  -1.30429123\n",
            " -1.28731413 -1.1795864  -0.36846217 -0.53154128 -1.18619516 -0.79149545\n",
            " -0.46440304  0.19256961 -0.35925862 -0.60004789 -0.71265051 -0.98805347\n",
            " -1.01673739 -0.97287782 -0.39569365  0.69705151 -0.63127784 -1.34046258\n",
            " -0.99462479 -0.74115563 -0.91363763 -0.7745028  -0.91923505 -0.72215443\n",
            "  0.69338532  1.79788392 -0.86805625  0.9150266   0.81053066  1.35885813\n",
            "  0.97006361  0.77734059  0.5450669  -0.22965416  0.57592788  1.58192072\n",
            "  1.43621287 -0.38467752  0.55433524  0.42894124  1.57069037 -0.22020388\n",
            "  0.2589168  -0.73750414  0.87525559  0.66910142  1.71497008  1.15345701\n",
            "  0.82253766 -0.83183847 -0.16793734 -0.85290685 -0.33071637  0.48903642\n",
            "  1.19298232  0.47440385  1.4645373  -0.25522213  2.16137973 -0.28472502\n",
            "  2.36975276  0.71588402  0.91256629 -0.2336732   1.40726986  1.07091856\n",
            "  0.43995298 -0.65010555  0.89578464  0.11880959  0.1191528   0.96550739\n",
            "  0.01533154  0.33072792  0.05602987  1.43808229 -0.9635518   0.40036756\n",
            "  1.14518506  5.67509595  0.44522122  1.51648913  0.14913309 -0.11655192\n",
            " -0.42202606  1.35494531  1.08049239  0.15657896  0.71562105 -0.72418524\n",
            "  0.45720684  1.10577187  1.86391924  0.88408276 -0.63758633  0.40184086\n",
            "  0.39233471  1.20002752 -0.54139123  0.87620549  1.66834189  1.31940085\n",
            "  0.54367193  1.44534087  0.38847696  1.78400981  1.45988288 -0.12338704\n",
            " -0.15428365  1.48650362  1.48183327 -0.40564728  0.16184028  1.02266595\n",
            "  1.1540993  -0.72545347  0.02819492 -0.29659451 -0.99404506  0.2230161\n",
            "  1.03851727 -0.88027641 -0.30926867  0.0771194   1.98534151  0.2500295\n",
            "  4.39982583  4.17122979 -0.46027718  1.34189587 -0.31147588  0.56209439\n",
            "  2.26211872 -0.87883635  0.10750152  0.96378254 -0.32762671 -0.6039418\n",
            " -0.08118765 -0.33280744  0.23614586  1.13308376  1.12143366  0.33731353\n",
            " -1.10831296  0.0969102   0.76324918  0.47746395  1.15010402  1.9403772\n",
            "  1.15188728 -0.13435702  1.48929238  1.24718729 -0.89954166  1.54927575\n",
            "  0.69934427  0.43073339  0.58122998  0.4924827   0.04878422 -0.21787553\n",
            "  0.72463586  0.75396522  0.02596414  0.26485267  0.82173184  0.64506885\n",
            " -0.33425037  1.08309985  2.27026354  0.67490019  1.64167412 -0.71575262\n",
            " -1.08811035 -0.41339028 -0.874587   -0.7132458  -0.82149311 -1.32460378\n",
            " -0.60000309 -1.1008539  -0.56788473  0.07921248 -0.86962274 -0.41194018\n",
            " -0.35560808  0.37845094  1.81004613 -0.17103939  0.05934865 -0.07420987\n",
            " -1.10353483 -0.91035413  0.15351804 -0.46174808 -0.92246298 -1.01223411\n",
            " -1.11403625  0.0576045  -0.05895952 -0.04156264  0.35446077 -0.33862439\n",
            " -0.97731818 -0.57224131 -1.06824352 -1.21217753  0.00717019 -0.73493993\n",
            " -0.78346942 -0.64808974 -0.75527867 -0.60715932 -1.16520195 -1.00994138\n",
            " -0.93003303  0.04990662  0.601215    0.26633801 -0.42149618 -0.63348565\n",
            " -1.02414082  0.04020952 -0.45076719  0.73267754 -0.94510496 -1.23810114\n",
            " -0.43346461 -0.7195283  -0.7942721  -0.51102294 -0.30369289 -1.13353603\n",
            " -0.58205912 -0.94514237 -1.05885649 -1.17407547 -0.91965314  0.25162775\n",
            " -0.58147212  0.16184028 -1.02186689 -0.69260869 -0.63674918  0.07638427\n",
            " -0.47254031 -0.68426189 -0.20022593  0.28223575 -0.06368858 -1.2600639\n",
            "  0.70563854 -0.87189164 -1.1567723  -0.59250975 -1.20891033 -0.68826732\n",
            " -0.79989455 -0.79214915 -1.04849745 -0.82659291  0.20553881  1.28340511\n",
            " -0.61055064  1.74552658 -0.42885674 -1.22031659 -0.84653297 -0.73081352\n",
            " -0.46051448 -0.0792959  -0.83945952 -1.23652421]\n",
            "[ 2.27991430e-01 -6.91279105e-01  1.94149985e-01  4.51669700e-01\n",
            " -7.09466870e-01 -4.82344389e-01 -3.51826901e-01 -6.58634588e-01\n",
            " -5.06381169e-01 -8.63020616e-01 -5.35942739e-01 -2.11503865e-01\n",
            " -2.02900911e-02  2.49527372e-02  4.00534646e-01 -5.64973528e-01\n",
            "  1.59256231e-02 -8.97362285e-01 -5.11798972e-01 -5.80860357e-01\n",
            " -4.66113079e-01  8.12607879e-02  4.25209403e-01 -5.34234185e-01\n",
            "  2.49052855e-01  6.82837917e-01 -6.58488952e-01 -3.04712903e-01\n",
            "  1.65560254e-01  4.78387766e-01 -3.84419695e-01 -6.42721187e-01\n",
            " -2.33082554e-01 -5.39172676e-01  1.39054233e+00 -7.81475322e-01\n",
            " -8.09448913e-01 -6.67055015e-02  9.57948224e-01 -1.04102555e-01\n",
            " -6.10697736e-01 -7.60656802e-01 -8.52050978e-02  6.44346958e-01\n",
            " -3.20332012e-01  2.75493140e-01  1.95152844e-01 -2.99636448e-01\n",
            " -1.90402827e-01 -1.96797974e-01  2.30147071e-02  2.97701268e+00\n",
            "  1.07868166e+00 -5.99239777e-01 -2.42730940e-01 -5.21928414e-01\n",
            " -7.93853344e-01 -3.87889291e-01 -4.72933633e-02 -4.16655875e-01\n",
            "  1.39849102e+00  1.49361172e+00 -7.18893404e-01  1.11810284e-01\n",
            "  1.66055152e-01  2.92635650e+00  1.90543428e-01 -7.57188450e-02\n",
            " -1.09755237e-01 -9.19521390e-01 -2.37719162e-01  1.52678366e+00\n",
            "  1.54062247e+00 -8.53944907e-03  2.82045274e-01  2.65861193e+00\n",
            "  2.01154935e+00 -7.87040659e-01 -2.33161272e-01 -5.89923218e-01\n",
            "  1.17517082e+00  8.69717109e-02  9.89583467e-01  4.67401063e-01\n",
            "  3.13594253e+00 -6.98257792e-01 -1.98412531e-01 -6.84049204e-01\n",
            " -2.82842243e-01  3.68745084e-01  9.45212440e-01 -2.63552878e-03\n",
            " -3.53186289e-01 -4.22902630e-01 -3.85878299e-01  1.63478954e+00\n",
            " -6.50376157e-02 -3.89324875e-02  2.15810907e-01 -4.82344389e-01\n",
            "  1.16034690e+00 -1.24394103e-01 -5.10280727e-01 -8.25316070e-01\n",
            "  1.03022873e+00  7.99498450e-01 -7.01278306e-01  1.60134330e+00\n",
            " -8.29240338e-01 -4.06430161e-01 -4.95809518e-01  1.05858770e+00\n",
            " -8.22727554e-01 -8.10342720e-02  2.14311466e+00 -7.60296698e-01\n",
            " -5.67740547e-01  2.46855621e+00  3.97043903e-01 -3.59674456e-01\n",
            " -5.89595812e-01  2.81941132e-01  4.29413841e+00 -6.68966107e-01\n",
            " -4.56853001e-01 -8.46700743e-01  1.43210737e-01 -2.67742711e-01\n",
            "  8.83969406e-01 -2.87534007e-01 -7.31992421e-01 -4.91406709e-01\n",
            "  8.70670306e-02 -5.99855945e-03 -2.51146401e-01  1.15870304e+00\n",
            "  2.66256663e+00  1.73660552e+00  4.57014985e-01  8.20213056e-02\n",
            " -3.84489198e-01  4.47662404e+00 -1.65778427e-01  3.74590209e-02\n",
            " -4.57628248e-01  3.19219323e+00  1.17338367e+00 -8.87777947e-01\n",
            " -7.01517986e-01  3.74034450e-01  4.62156703e-02 -8.84117093e-01\n",
            " -4.70134938e-01 -4.01859967e-01 -8.43955031e-01 -4.41868507e-01\n",
            "  6.92714711e-01 -9.14550713e-01 -9.19405984e-01 -5.66612461e-01\n",
            "  1.90677915e+00 -8.02878418e-01  2.41612417e+00  2.25586148e+00\n",
            " -7.24246512e-01  7.21621958e-03 -8.13055608e-01 -3.11251456e-01\n",
            "  4.70647715e-01 -9.16940033e-01 -4.00149481e-01 -4.74772786e-01\n",
            " -5.11009836e-01 -7.34273790e-01  5.34200477e-01 -7.38479382e-01\n",
            " -5.21628781e-01 -8.45106822e-02  6.46248505e-01  4.59377713e+00\n",
            " -8.74407944e-01 -3.96651344e-01  1.32473918e+00  2.25603077e+00\n",
            "  5.66073123e-02  4.35585628e-01  9.97847618e-02 -8.57112403e-01\n",
            "  2.45091716e+00  1.01420989e+00 -7.02444397e-01  2.74557428e+00\n",
            "  1.57981906e+00 -6.78436209e-01 -1.11208047e-01 -8.57050605e-01\n",
            " -5.52128724e-01  2.26653597e+00 -4.83160745e-01 -2.92584637e-01\n",
            " -1.48761945e-02  1.57876779e+00  3.04803393e+00 -7.06880381e-01\n",
            " -4.53291470e-01  1.67977843e+00  3.96962412e+00 -1.20152382e-01\n",
            "  1.62013203e-02 -3.11547833e-01 -2.70348141e-01 -1.07873194e-01\n",
            " -7.39121804e-01 -2.32452653e-01 -6.63273508e-01 -5.17241298e-01\n",
            " -8.19082154e-01 -8.22647671e-01 -3.44285115e-01  6.44346958e-01\n",
            "  1.51257303e-01 -8.42050514e-01 -5.32107941e-01 -1.13853931e-01\n",
            "  2.36760778e-01 -7.85029817e-01 -3.37636181e-01 -4.60506536e-02\n",
            " -7.64171755e-01 -6.89687258e-01  2.88302055e-01 -5.41921988e-01\n",
            " -4.88978585e-01 -7.30834121e-01 -7.20884940e-01 -1.97494325e-02\n",
            " -5.02524489e-01 -4.79890746e-01 -5.21029178e-01 -6.66177586e-02\n",
            " -7.43530993e-01 -5.07357792e-01 -8.50509536e-01 -7.92254349e-01\n",
            "  7.66400636e-01 -6.61876552e-01 -3.52542571e-01 -2.87685065e-01\n",
            " -7.53814843e-01  4.20037866e-03 -9.18193426e-01 -9.12073891e-01\n",
            " -8.42563160e-01  1.08339704e-01 -5.49077468e-02 -4.35080669e-01\n",
            " -7.05070991e-01 -8.99772609e-01 -6.85154673e-01 -2.72566298e-01\n",
            " -5.98271043e-01 -4.17262105e-01 -4.53356346e-01 -8.89200551e-01\n",
            " -8.14730308e-01 -7.11981263e-01 -6.84187530e-01 -7.19862811e-01\n",
            " -3.06202006e-01 -3.06797142e-01 -7.41932501e-01 -3.02326582e-01\n",
            " -8.08757080e-01 -9.16883596e-01 -6.95090489e-01 -2.89043779e-01\n",
            " -5.00435664e-01 -5.26111424e-01 -9.15688020e-01 -1.87154038e-01\n",
            " -5.12708564e-01 -4.45408050e-01 -5.11677616e-01 -5.31634648e-01\n",
            " -4.41145875e-01 -3.01579910e-01 -2.58814335e-01 -9.19437826e-01\n",
            " -3.61805221e-01 -4.66368967e-01 -4.85041732e-01 -4.64063370e-01\n",
            " -9.04048525e-01 -8.49229429e-01 -6.91188291e-01 -8.20768410e-01\n",
            " -3.26859923e-01 -1.25664419e-01  6.57338210e-02  1.09671814e+00\n",
            " -8.25470676e-01  6.14549976e-01  1.22257114e-01 -9.10075503e-01\n",
            " -5.77923186e-01 -5.01296400e-01  4.91122448e-02  2.42488994e-01\n",
            " -7.49159094e-01 -8.61584769e-01]\n",
            "[-0.67561195 -1.10875127 -0.24375262 -0.49439136 -1.02787439 -1.10621826\n",
            " -0.76175152 -1.0088826  -0.55912107 -0.58796398 -0.94519378 -1.28579643\n",
            " -0.65158488 -0.89296306 -0.38162661 -0.79765367 -0.02940995 -0.90915708\n",
            " -0.95478417 -0.97940279 -0.7387773  -0.57826374 -0.05692919 -0.97454671\n",
            " -1.00989547 -0.82288036 -1.30611873 -0.38162661 -1.06661887 -0.28085567\n",
            " -0.56546146 -0.92181531 -1.04168369 -0.66131648 -0.40053273 -1.12527961\n",
            " -0.80099791 -1.19934517 -0.3515765  -0.80512385 -1.28451138 -1.00890927\n",
            " -0.90267513 -0.35992654  0.38496042 -0.38162661 -0.79489841 -0.96311588\n",
            " -0.8652283  -1.21185959 -0.8652283  -1.04374759 -0.79748787 -0.8652283\n",
            " -0.8816818  -0.77823401 -1.03439969 -0.73296117 -0.8335393  -0.81522893\n",
            "  1.45359264  1.77770918 -0.15429249  0.51030208  1.76481786  0.36926489\n",
            "  0.25720524  0.49376785 -0.20159636 -0.17805924 -0.15429249 -0.34705969\n",
            "  0.30494063 -0.50700483  2.40700533  0.73486498  0.80443607 -0.08951424\n",
            "  0.26520604 -0.69759809  0.82099817  0.11596618 -0.35713235  1.38991923\n",
            "  0.1064942  -0.46613061  0.2137364  -0.48132013 -0.5859977  -0.5192084\n",
            "  0.16125751  0.4185224   0.96283455  1.5174733   0.70916497 -0.20580865\n",
            "  2.34134229  1.74754358  2.65266187 -0.60639116  0.57294683  0.44477167\n",
            "  0.32400014 -0.1060701   0.44012233 -0.20403103  0.13478728  1.27592883\n",
            "  0.4266725   0.42754061  0.37633355  1.79460665  0.72124701 -0.3158375\n",
            "  2.07689575 -0.32099892  0.39309221 -0.34367668 -0.43382132 -0.53908255\n",
            " -0.62411635  2.09136827 -0.13496936  0.06937082 -0.91103957  0.09899045\n",
            "  0.21237714  0.63703202  0.8423011  -0.2582461  -0.9208998   0.14881968\n",
            " -0.11738799  0.89039444  1.40481961  1.36094933  1.67493758  1.4283971\n",
            "  1.53388424  2.46868876  1.14863785  0.98487285  3.26469685  0.51658026\n",
            "  0.15089807  1.45207341  1.41458365  0.0441082  -0.20987346 -0.63174228\n",
            "  0.29528346  0.16725396 -0.33194801  0.48499863 -0.55220543  0.99029912\n",
            "  0.89629603 -0.4196846  -0.28451903  0.77156124  1.35639646 -0.16872255\n",
            " -0.20912643  1.31269692 -0.54300629  1.33468005  0.09647592  0.24999354\n",
            "  2.76249389 -0.73296117  0.19389411  1.52069324 -0.51627597  0.0595807\n",
            " -0.694119   -0.12653197  0.61351015  0.34893624  0.30706554  0.73098753\n",
            " -1.11571648  0.70456547  2.29088581  1.88597589  2.47281863  1.89242393\n",
            "  1.45323088  0.24697043  1.74344125  1.33071511 -0.15429249  0.32068514\n",
            "  0.66192032  1.28244858  0.72748836  2.00055947  0.56585624  5.86503184\n",
            "  0.40895066  0.32940087  0.69631144  2.86140541  2.60039605  0.4266725\n",
            " -0.46646125  1.92999422  0.2025532   1.25879119  2.64654095 -0.7900031\n",
            " -0.82990068 -0.17327686 -0.38585291 -0.33799683 -0.56354623 -0.80410196\n",
            " -0.95771214 -0.22745365 -0.8335393  -0.20351621 -0.94118422 -0.1060701\n",
            "  0.40572734  1.25418395  2.37056618  0.01051219  0.43385505 -0.11132236\n",
            " -1.07998779 -0.79987317  0.40256439 -0.69759809 -0.6485121  -0.95415751\n",
            " -0.5859977  -0.65479896  0.48499863  0.14881968  0.02207512 -0.52397815\n",
            " -0.79987317 -0.8335393  -1.06301697 -0.54696052 -0.17308727 -0.05692919\n",
            " -0.66131648 -1.03607334 -0.81539225 -0.76740574 -0.8768131  -0.2479817\n",
            "  0.14881968  0.60440645  0.31316804  0.14887295 -0.33799683 -0.5859977\n",
            " -0.73407762 -0.27768495 -0.54696052 -0.07028775 -1.10160758 -1.09749948\n",
            " -0.38162661 -0.67189623 -0.55876812 -0.38214459 -0.5531895  -1.1315996\n",
            "  0.09600468 -0.5859977  -0.76740574 -0.52881474 -1.01154521 -0.20159636\n",
            " -0.88811737 -0.50700483 -0.9552681  -0.15429249 -0.85082874 -0.51092076\n",
            " -0.60196663 -0.16030335 -0.53908255 -0.53677961 -0.38593907 -1.22909928\n",
            "  0.18197242 -0.91868119 -1.20793838 -0.80255095 -0.9963537   0.01850441\n",
            " -0.76740574 -0.33358334 -0.77840245 -0.87226276 -0.93384071  0.16222045\n",
            " -0.8335393   0.79173824 -1.00024658 -0.91428798 -0.48328681 -0.80093178\n",
            " -1.00398623 -0.41714176 -0.60200483 -0.77954727]\n",
            "[-2.87134567e-01 -8.79719944e-01  1.10152347e-01 -2.34396666e-02\n",
            " -3.11910712e-01 -9.89403803e-01 -4.55665046e-01 -6.14321729e-01\n",
            " -8.24242444e-01 -7.58939342e-01 -5.05477712e-01 -1.36871895e+00\n",
            " -8.81657645e-01 -6.89807372e-01 -6.89807372e-01 -7.53941047e-01\n",
            " -1.00180358e-01 -9.16439790e-01 -1.03060801e+00 -8.58863872e-01\n",
            " -8.47026063e-01 -6.52683824e-01 -3.93865071e-01 -7.20996839e-01\n",
            " -1.37171555e+00 -9.43586833e-01 -1.32853153e+00 -9.59881005e-01\n",
            " -1.12427462e+00 -1.94483809e-01 -5.72379637e-01 -5.77950732e-01\n",
            " -9.42810767e-01 -1.01456141e+00 -9.38553318e-01 -1.08857898e+00\n",
            " -1.03625820e+00 -1.30802435e+00 -9.62934500e-01 -6.75126715e-01\n",
            " -1.08388953e+00 -4.28806350e-01 -6.13652905e-01 -3.18035254e-01\n",
            " -3.31418319e-01 -9.26182153e-01 -9.98092465e-01 -1.04837788e+00\n",
            " -1.12561814e+00 -1.08116256e+00 -6.02192668e-01 -7.01330479e-01\n",
            " -1.19372553e+00 -1.25748046e+00 -1.08180082e+00 -6.26164348e-01\n",
            " -5.47743097e-01 -7.66967431e-01 -1.09271753e+00 -6.86457716e-01\n",
            " -1.73307541e-01  9.07788343e-01 -5.89199310e-01  8.51723089e-01\n",
            "  6.79337654e-01 -1.52975779e-01  8.51723089e-01  8.51723089e-01\n",
            "  5.93112366e-01  1.07004480e+00  7.57777431e-01  6.60204644e-01\n",
            "  5.01204592e-01 -5.72379637e-01  2.90252910e-01 -8.17797102e-01\n",
            "  4.06453745e-01  4.37098863e-01  3.51015554e-01 -5.59810114e-01\n",
            "  1.11746114e-01  5.73654312e-01  1.12925435e+00  8.51723089e-01\n",
            " -6.49006663e-01 -5.65999769e-01 -1.99533375e-01 -6.09995660e-01\n",
            " -3.27044612e-01  1.57393524e-01  5.75540996e-01  4.12638754e-01\n",
            "  2.08556031e+00 -1.13586569e-01  3.10900140e+00 -1.08091422e+00\n",
            "  2.92376904e+00  7.40426441e-01  7.61551088e-01 -2.64238809e-02\n",
            "  6.80385160e-01  1.27544883e+00  9.03810168e-01 -1.10305420e-01\n",
            "  2.08525391e-01 -4.64801925e-01  7.57777431e-01  2.74864828e-04\n",
            "  8.96454973e-01  6.24895254e-01  3.67688141e-01  7.75277534e-01\n",
            "  2.48934511e-01  3.88858789e-01 -5.63629420e-02  9.45260315e+00\n",
            "  9.94845768e-01  1.57393524e-01 -2.32148626e-01  2.74864828e-04\n",
            " -1.52975779e-01  1.24273037e+00 -7.43323683e-01  7.53597271e-01\n",
            "  1.20607507e+00 -1.70223518e-01  2.77639466e-01  1.48571772e+00\n",
            "  1.37421127e+00  1.21532821e+00 -2.58637468e-01  8.26184397e-01\n",
            "  2.43740467e-01  1.31806171e+00 -5.94615830e-01  1.21047168e-01\n",
            "  2.20462613e-01  2.85946218e-01  1.61753942e-01  1.54169453e+00\n",
            "  6.75391623e-01 -2.79543442e-01  1.83235207e+00 -3.13755679e-01\n",
            "  4.98319217e-02 -1.33383816e-01  7.53597271e-01  1.06829553e+00\n",
            "  8.19447449e-01  7.68368033e-01  1.20840933e+00 -6.18687459e-02\n",
            "  3.00785883e-01 -1.85044388e-01 -5.73537276e-01  5.24435025e-01\n",
            "  5.62822492e-01 -1.49043276e-01  8.83547263e-01  4.86791961e-01\n",
            "  8.78915737e-01  1.17671383e+00  3.21511406e+00  3.06391052e+00\n",
            " -2.40367503e-02  1.48571772e+00  3.61948528e-01  8.19447449e-01\n",
            "  2.19515954e+00 -1.20778666e-01  3.26220259e-01  1.57346340e+00\n",
            " -1.17056515e-01 -2.08610435e-01 -5.39995009e-01  1.79394233e-01\n",
            "  6.42034369e-01  1.31330078e+00  6.87321136e-01 -1.22798094e+00\n",
            " -6.74282442e-01  3.08852666e-01 -7.25227095e-02 -6.71249454e-01\n",
            "  1.19343373e+00  1.82737439e+00  1.15515346e+00  7.55973751e-01\n",
            "  1.39170113e-01  5.93112366e-01 -6.50216467e-01  7.94711893e-02\n",
            " -2.43989851e-01  1.16262737e+00  6.39558909e-01  1.72977742e+00\n",
            "  4.28978804e-01 -1.16890436e+00  1.25557713e+00  1.04961553e+00\n",
            " -1.08695601e-01 -6.33455751e-01 -6.26164348e-01  1.53209150e+00\n",
            " -1.83791592e-01  7.94711893e-02  2.74583953e-01  7.65574311e-01\n",
            "  1.86249628e+00 -7.51057100e-01 -1.15968281e+00 -5.44006542e-01\n",
            " -5.72379637e-01 -7.96490769e-01 -5.92013740e-01 -1.27984613e+00\n",
            " -5.03864874e-02 -7.63920938e-01 -5.59810114e-01 -4.32422183e-01\n",
            " -1.12147925e+00  2.85946218e-01 -1.30701261e-01  3.90584601e-01\n",
            "  1.85070195e+00  5.04912473e-01  2.00400433e-01 -2.02147830e-01\n",
            " -8.47026063e-01 -6.78449965e-01 -1.63918520e-01 -2.57054777e-01\n",
            " -8.66463764e-01 -7.65515271e-01 -9.07416739e-01 -6.86055974e-02\n",
            "  2.23686319e-01  2.20462613e-01  8.02026698e-01 -4.87029342e-01\n",
            " -7.04503230e-01 -4.30784513e-01 -6.68011496e-01 -9.57343311e-01\n",
            " -5.57954554e-01 -4.80750442e-01 -8.00293202e-01 -6.90742077e-01\n",
            " -3.88918362e-01 -8.12410327e-01 -4.88953199e-01 -3.79396807e-01\n",
            "  2.62533058e-01 -1.65622334e-01  6.12819750e-01  5.73654312e-01\n",
            "  2.74864828e-04  5.93112366e-01 -8.28776436e-01  1.15074767e-01\n",
            " -1.80719730e-01  1.17546416e+00 -9.15979813e-01 -8.30612641e-01\n",
            "  1.85129610e-01 -3.98628798e-01 -5.32968144e-01 -1.01472211e-01\n",
            " -2.76162034e-01 -1.18734104e+00 -1.66316170e-01 -1.00316211e+00\n",
            " -7.26969401e-01 -6.06382592e-01 -6.84681586e-01  3.95992227e-01\n",
            " -4.48674117e-01  5.47486812e-01 -2.09665294e-01 -8.00293202e-01\n",
            " -5.06022563e-01  3.35748551e-01 -3.01468784e-01 -5.47743097e-01\n",
            " -2.60261595e-02  4.47596586e-01 -2.49653077e-02 -6.77210726e-01\n",
            "  1.06829553e+00 -8.24242444e-01 -1.12685877e+00 -4.94330314e-01\n",
            " -7.37605134e-01 -1.10305420e-01 -5.32108153e-01 -3.27044612e-01\n",
            " -1.09592985e+00 -9.70132220e-01  4.13471900e-02  5.83726519e-01\n",
            " -5.17667615e-02  1.43913952e+00 -6.89807372e-01 -7.28266848e-01\n",
            " -7.06598677e-01 -6.30909765e-01 -6.84681586e-01 -3.88572832e-01\n",
            " -5.11245569e-01 -8.89506625e-01]\n",
            "[-1.39329781e+00 -3.18185125e-01 -9.13171362e-01 -1.18774028e+00\n",
            " -7.63486428e-01  9.31012807e-01  1.45897008e-01 -1.05744381e-01\n",
            "  4.94685602e-01 -2.23966088e+00 -7.30118917e-01  1.06550773e-01\n",
            "  4.97567256e-02 -5.66759900e-01 -1.40267130e-01  4.24398684e-01\n",
            " -4.70833564e-01  8.21257210e-01  3.48288378e-01 -1.56730047e-01\n",
            "  1.28163134e+00  5.11532006e-01 -8.83887614e-01 -5.21929326e-02\n",
            " -2.31911499e-01  2.14180727e-01  5.10729155e-01 -6.86800556e-01\n",
            " -5.59640975e-01 -3.86135437e-01 -4.79470920e-01 -1.38684073e+00\n",
            " -5.21929326e-02  4.87474372e-01  5.05218723e-02 -3.60627824e-01\n",
            "  1.09349985e+00  1.55022939e+00  3.60141659e-01 -1.58972585e-01\n",
            " -8.48316327e-01 -2.39319619e-01  2.31329246e-01  1.96285914e-01\n",
            " -3.46747939e-01 -9.44576261e-02 -9.74688380e-02 -4.01402301e-01\n",
            "  1.83081115e-01 -2.33987957e-02 -2.09648666e-01 -7.78730496e-01\n",
            "  7.61861440e-01 -4.44143588e-01 -1.72415697e-01 -1.42000450e+00\n",
            " -4.36915012e-01 -1.82111690e-01  4.82669770e-01  4.19624476e-01\n",
            "  2.09278040e+00  9.00135285e-01  1.30320630e+00 -2.88052391e-01\n",
            "  4.09288280e-01 -2.74051890e-01 -1.04872307e-02 -1.56603971e+00\n",
            " -3.32844905e-01 -1.17662628e+00  3.98156545e-02  1.33521078e+00\n",
            "  5.47726664e-01  3.08095144e-01  1.56788115e+00  2.55821248e+00\n",
            "  1.71627455e+00 -1.40417111e+00 -5.56079107e-01 -8.76786480e-02\n",
            " -4.82347987e-01  1.18105882e-01  2.38353494e-01  2.87697539e+00\n",
            "  4.36344301e-01  4.01080932e+00  4.97567256e-02  9.10139998e-01\n",
            "  6.79918771e-01  9.11664042e-02 -2.91732885e-01 -1.87328121e-01\n",
            "  4.48304346e-01 -3.78127223e-01 -2.82897005e-01  1.85741302e+00\n",
            "  1.53631514e-01 -6.80491919e-01  3.40394207e-01 -2.37097856e-01\n",
            " -7.74576101e-01 -2.90260880e-01 -1.15696811e+00 -5.74583312e-01\n",
            " -1.65655697e-02 -1.57477623e-01 -1.63724791e+00  1.63963305e+00\n",
            " -5.90206860e-01  3.69634671e-01 -3.38701636e-01  6.48948688e-01\n",
            " -5.22519195e-01 -1.52983083e+00  1.86736095e+00 -7.96706200e-01\n",
            " -3.34309473e-01  2.64153508e-01  1.45360322e+00 -4.17365828e-01\n",
            " -7.78776213e-02  4.66671104e-01 -5.85239097e-01 -6.94504058e-01\n",
            " -8.31840329e-01 -1.59426023e+00 -1.11285652e+00 -1.67936968e-01\n",
            " -1.25144793e+00 -5.86658779e-01 -1.32721209e+00  1.99660225e-02\n",
            " -1.41016118e-01  5.81768654e-02  1.50006261e+00 -5.92334939e-01\n",
            " -1.55982405e-01 -6.02258344e-01 -8.32527567e-01 -7.01500509e-01\n",
            " -9.75426758e-01  1.29629795e+00 -5.88787820e-01 -1.08751745e-01\n",
            " -9.71383167e-01 -9.70709011e-01 -2.84370293e-01 -6.41121477e-01\n",
            " -1.02109279e+00 -1.39646061e+00 -1.68082128e+00  5.22778651e-01\n",
            " -4.98871211e-01 -1.45508697e-01 -1.09036143e+00 -2.33506107e+00\n",
            " -5.92578323e-03 -7.31512179e-01 -1.11021389e+00 -1.60599909e+00\n",
            " -2.21529359e-01 -1.44822720e+00 -2.41006606e+00 -1.00031122e+00\n",
            "  4.46708840e-01  5.33233219e-01 -6.36891289e-01 -2.94017778e+00\n",
            " -1.95772116e+00 -1.41849229e+00 -5.16076670e-01 -9.98296835e-01\n",
            " -1.70050108e+00 -2.51401206e+00 -1.08704708e+00 -1.38671405e+00\n",
            " -1.26243658e+00 -1.94832077e+00  1.09630725e-01  4.96288803e-01\n",
            "  3.47371504e+00 -1.90071271e+00 -8.55855482e-01  5.07518392e-01\n",
            "  5.29210949e-01 -1.02735990e-01 -1.91378212e+00 -7.03598194e-01\n",
            "  1.66794890e-01 -1.23022716e-01 -2.00752751e+00 -1.47307367e+00\n",
            "  2.36337502e+00  4.36378609e-02 -1.29024620e-01 -5.85948970e-01\n",
            " -1.92814368e+00  1.38294724e+00 -2.59289514e-01  1.61537551e-02\n",
            "  9.87149563e-01  2.28190657e+00  1.09434797e+00 -1.38981688e+00\n",
            " -1.19753137e+00  5.42087855e-01 -1.51787743e+00  7.71737651e-01\n",
            "  1.74546708e-01 -1.62855419e+00  7.07707333e-01  5.63047840e-01\n",
            "  8.38643593e-01  8.18775750e-01  9.18484312e-01  1.98734133e+00\n",
            " -3.93227910e-02  1.08078572e+00  8.52738995e-01 -9.00246652e-01\n",
            " -3.64276578e-01  6.53832545e-01 -1.83602420e-01 -1.07510214e+00\n",
            " -1.27662967e+00  5.08320987e-01  1.98277626e+00  1.68175900e-03\n",
            "  3.98962920e-01  1.54141313e+00  4.25194609e-01 -3.62918292e-02\n",
            "  7.80798956e-01  1.69037733e+00  1.39251781e+00 -4.15915881e-01\n",
            "  2.39915143e-01  1.18033856e+00  6.24564027e-01  1.69662344e+00\n",
            " -1.58972585e-01  6.33498368e-01  1.03597801e+00  4.14057485e-01\n",
            " -3.10845617e-01  5.25994287e-01  5.43485673e-02  2.13401987e-01\n",
            "  1.46670170e-01  2.83745462e-01  1.40993872e+00  1.70197976e+00\n",
            "  2.26649299e-01  6.73713215e-02 -5.48237356e-01  3.85476878e-01\n",
            "  3.38816142e-01  1.05455557e+00  7.38852857e-01 -1.19269442e-01\n",
            "  2.73552644e-01 -9.36242016e-01  8.12987920e-01  7.65152484e-01\n",
            " -5.08195419e-01  1.38207757e+00  1.98618097e-01  3.42761785e-01\n",
            " -3.62087518e-01  4.95487171e-01  6.13204268e-01  1.50536942e-01\n",
            " -6.42890786e-02  6.49762503e-01  3.72801061e-01  1.55463993e+00\n",
            "  1.01322069e+00  8.52738995e-01  5.12870830e-02  1.69305385e+00\n",
            "  3.52311228e-02 -2.45981445e-01 -3.54786483e-01  6.07529102e-01\n",
            "  1.82304940e-01  7.81098572e-02  1.72995832e-01  1.31991057e-01\n",
            " -9.51806602e-01  1.04779650e+00  5.81625498e-01 -2.15591064e-01\n",
            "  6.67682677e-01  1.51236497e+00  7.42957845e-01  9.32684363e-01\n",
            "  1.06470186e+00 -1.41016118e-01  8.12161343e-01 -5.85239097e-01\n",
            " -1.56730047e-01 -5.30386339e-01 -2.49165850e-02  7.62684105e-01\n",
            "  2.49738908e+00 -9.21985438e-02 -3.21119133e-01  5.48532465e-01\n",
            " -8.96700467e-03  4.10877758e-01]\n",
            "[-2.04026068e-01 -2.02005990e-01 -2.02820108e-01 -1.91804876e-01\n",
            " -1.97933388e-01 -2.03548354e-01 -2.00544001e-01 -1.92946024e-01\n",
            " -1.86855034e-01 -2.03989421e-01 -1.98075046e-01 -2.04008071e-01\n",
            " -1.97762138e-01 -1.99205018e-01 -2.00797500e-01 -2.01179399e-01\n",
            " -2.03994668e-01 -2.02755608e-01 -2.03833682e-01 -2.03881740e-01\n",
            " -1.99983754e-01 -2.03456774e-01 -1.79811756e-01 -2.01613189e-01\n",
            " -2.03012792e-01 -2.03792240e-01 -1.94123780e-01 -1.96166237e-01\n",
            " -2.03348111e-01 -2.03640992e-01 -1.92539450e-01 -2.00064674e-01\n",
            " -2.01015060e-01 -2.03321701e-01 -2.03820945e-01 -2.02155152e-01\n",
            " -2.00031556e-01 -2.04025610e-01 -2.03263011e-01 -2.03753008e-01\n",
            " -2.02867638e-01 -2.03625386e-01 -2.01269287e-01 -2.03970171e-01\n",
            " -2.03506251e-01 -2.03411382e-01 -1.99945888e-01 -2.02064432e-01\n",
            " -2.03802460e-01 -2.03618393e-01 -2.02800732e-01 -2.01600560e-01\n",
            " -2.00826938e-01 -1.91251281e-01 -1.95125551e-01 -2.00204807e-01\n",
            " -2.00812740e-01 -2.03763458e-01 -2.03203527e-01 -1.97888015e-01\n",
            "  1.02375016e-01  6.19729278e-01 -1.21625017e-01 -1.33480539e-01\n",
            " -1.93446990e-01 -1.35271333e-01 -1.31097850e-01  2.66487317e-01\n",
            " -2.03933140e-01 -1.27446896e-01 -1.03749992e-01  8.02398019e-01\n",
            "  3.33413046e-01 -8.79028634e-02  1.09822508e-01  1.85176528e+00\n",
            "  9.91388371e-01  5.47895643e-02 -9.70905140e-02 -1.58305679e-01\n",
            "  2.82973746e-01  3.36628066e-01  5.70221213e-02  2.09649073e-01\n",
            "  4.03750998e-02 -1.63889838e-01 -1.56422346e-01 -6.82319759e-02\n",
            " -1.47917116e-01 -1.31624269e-01 -1.12900696e-01 -1.01153649e-01\n",
            "  3.58099486e-01  4.14587181e-02  7.18295943e-02  1.94817932e+00\n",
            "  1.95387443e-01 -9.61337182e-02  1.60410949e-01 -1.14211904e-01\n",
            " -1.42664479e-01 -5.95642774e-02 -1.40595047e-01 -1.41594629e-01\n",
            "  6.47475370e-01 -1.99244407e-01  3.61999270e-02  3.74995132e-01\n",
            " -2.74475568e-02 -5.22849675e-02 -3.71332100e-02  2.90740513e-01\n",
            " -1.31332049e-01  5.06517584e-02  5.13223640e-01  1.68350019e+01\n",
            "  1.10906386e-01  1.69722011e-01 -1.77941241e-01 -1.06325484e-01\n",
            " -4.54164761e-02  4.27167393e-01  2.42131830e-01  7.88879221e-02\n",
            " -7.07497579e-02 -9.91574706e-02 -1.28805490e-01  1.20711642e-01\n",
            "  1.32541309e-01 -5.00562249e-02 -1.34342368e-01 -1.72814393e-01\n",
            "  8.82492957e-02  9.56537000e-02  8.95517399e-02  4.51499596e-01\n",
            "  1.09402731e-01  3.59582908e-02  1.05659336e-01 -6.33329772e-02\n",
            " -1.18227248e-01  1.15837681e+00  1.33239719e-01  8.65492149e-01\n",
            " -1.14087718e-01 -2.51274073e-02  2.50723616e-01 -1.05831610e-01\n",
            " -5.37490252e-02 -7.29799097e-02 -4.44014629e-02 -8.48787753e-02\n",
            " -1.05769789e-01 -1.14489533e-01 -1.29614810e-01 -3.50152627e-02\n",
            "  1.66269111e-01 -2.57692482e-02  6.06527154e-02 -5.29603888e-02\n",
            "  1.23832727e-01 -3.13300081e-02  3.29242091e-01  4.37284112e-01\n",
            " -1.42840263e-01  2.62827774e-01 -7.22362295e-02 -1.89854805e-01\n",
            "  1.19364182e-01 -1.48420619e-01 -1.59458256e-01 -1.89093281e-01\n",
            " -1.60694728e-01 -1.52389373e-01 -1.57624069e-01  4.60143887e-02\n",
            " -9.16630284e-02 -1.42803013e-02  6.37364880e-02  8.69071878e-02\n",
            " -9.41746569e-02 -1.08581267e-01  8.98403771e-02  2.42711735e-01\n",
            " -6.22283957e-02  6.53209005e-02  3.10267618e-02 -1.14931668e-01\n",
            "  1.69372384e-01  1.79150521e-01 -4.14722262e-02  9.83050818e-02\n",
            "  1.31529751e+00 -1.00234231e-01 -1.41357824e-01 -6.16484362e-02\n",
            "  1.00946354e-01  1.13453769e+00 -9.70001887e-02 -1.07051675e-01\n",
            " -6.68327700e-02  5.54374544e-01  7.92528211e-01 -1.33249824e-01\n",
            " -1.39422179e-01  7.74299726e-01  3.76917937e-01  3.19834756e-01\n",
            "  3.68648607e-01 -1.25888781e-01 -1.97827878e-01 -2.03642668e-01\n",
            " -2.01606296e-01 -2.03951649e-01 -2.02915272e-01 -2.03641187e-01\n",
            " -2.03923485e-01 -2.00124828e-01 -2.03933297e-01 -2.03086608e-01\n",
            " -2.04024511e-01 -2.03993561e-01 -1.09511379e-01 -2.04013218e-01\n",
            " -2.03132858e-01 -2.03014304e-01 -2.00173013e-01 -2.01777275e-01\n",
            " -2.02563667e-01 -2.03088851e-01 -2.02410577e-01 -2.01527751e-01\n",
            " -2.03940668e-01 -2.03418782e-01 -1.85821368e-01 -1.92138465e-01\n",
            " -2.03868940e-01 -2.01644135e-01 -1.92619665e-01 -2.03695963e-01\n",
            " -2.00466805e-01 -1.85163378e-01 -2.03976261e-01 -2.02410895e-01\n",
            " -1.98648361e-01 -2.04016604e-01 -1.96040127e-01 -2.03676292e-01\n",
            " -2.03773709e-01 -2.03522947e-01 -2.03606469e-01 -2.03988887e-01\n",
            " -2.02932936e-01 -1.89117380e-01 -2.03967995e-01 -1.38430687e-01\n",
            " -1.97435615e-01 -1.97347369e-01 -2.04011880e-01 -2.04019910e-01\n",
            " -2.04030070e-01 -2.03785004e-01 -2.03976621e-01 -2.03725528e-01\n",
            " -2.02143425e-01 -2.01237782e-01 -2.04003787e-01 -2.03697047e-01\n",
            " -2.01585850e-01 -1.98754215e-01 -2.03585247e-01 -1.97997608e-01\n",
            " -2.03679745e-01 -2.03773233e-01 -2.03737535e-01 -2.01639120e-01\n",
            " -2.03721089e-01 -2.03650224e-01 -2.00325026e-01 -1.93424654e-01\n",
            " -2.03103937e-01 -2.00587088e-01 -2.03303711e-01 -2.04004474e-01\n",
            " -2.03253076e-01 -2.01727830e-01 -2.03105019e-01 -1.95520926e-01\n",
            " -2.03997434e-01 -2.02768754e-01 -2.04029222e-01 -1.99394574e-01\n",
            " -2.04011702e-01 -1.65687180e-01 -2.01502317e-01 -2.03448797e-01\n",
            " -2.01630717e-01 -2.03078279e-01 -2.03808462e-01 -2.00437535e-01\n",
            " -2.04027879e-01 -2.00458555e-01 -2.03821862e-01 -2.02930646e-01\n",
            " -1.96255379e-01 -2.02279257e-01 -2.04015123e-01 -2.03319122e-01\n",
            " -2.04027878e-01 -2.04028502e-01]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(310,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwnmcLuQsdWb"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYrl86Dhy9jy"
      },
      "source": [
        "# sigmoid function\n",
        "def sigmoid(a):\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "class Logistic_regression():\n",
        "  def __init__(self):#,X_train,y_train,learning_rate,X_test,y_test):\n",
        "    pass\n",
        "    \n",
        "  # training\n",
        "  def fit(self,X_train,y_train,learning_rate):\n",
        "    #n,m = np.shape(self.X_train)\n",
        "    n,m = np.shape(X_train)  \n",
        "    wk = np.ones([m+1,1]) # wk weights, initialized with 1\n",
        "    wk1 = np.zeros([m+1,1])# wk+1 weights,initialized with 0          \n",
        "    itrnum = 500       # max number of iterations \n",
        "    e = 0.001\n",
        "    der = 0\n",
        "    for k in range(0,itrnum):\n",
        "      for i in range(0,n):\n",
        "        #xi = self.X_train[i].T\n",
        "        xi = X_train[i].T\n",
        "        x0 = np.array([1])\n",
        "        xi = np.concatenate((xi, x0),axis = 0)\n",
        "        #yi = self.y_train[i]\n",
        "        yi = y_train[i]\n",
        "        der = der-xi*(yi-sigmoid(np.matmul(wk[:,0].T,xi))) # take derivative w.r.t w\n",
        "      #wk1[:,0] = wk[:,0]-self.learning_rate*der       # update rule\n",
        "      wk1[:,0] = wk[:,0]-learning_rate*der       # update rule\n",
        "      if (np.linalg.norm(wk1[:,0]-wk[:,0]))**2<e:         \n",
        "        break \n",
        "      else:\n",
        "        wk = wk1.copy()\n",
        "    return wk1\n",
        "  \n",
        "  # validation\n",
        "  def predict(self,w,X_test):\n",
        "    #n,m = np.shape(self.X_test)\n",
        "    n,m = np.shape(X_test)   \n",
        "    y_predict = np.zeros([n,1])\n",
        "    for i in range(0,n):\n",
        "      #xi = self.X_test[i].T\n",
        "      xi = X_test[i].T\n",
        "      x0 = np.array([1])\n",
        "      xi = np.concatenate((xi, x0),axis = 0)\n",
        "      p1 = sigmoid(np.matmul(w.T,xi)) # calculate probabilities p(y=1|x)\n",
        "      # covert probabilities to 0 or 1 by thresholding at 0.5\n",
        "      if p1>=0.5:\n",
        "        y_predict[i] = 1\n",
        "      else:\n",
        "        y_predict[i] = 0\n",
        "    return y_predict\n",
        "\n",
        "  # evaluate accuracy\n",
        "  def Accu_eval(self,y_test,y_predict):\n",
        "    #y_predict = self.predict(X_test)\n",
        "    n,j = np.shape(y_predict)\n",
        "    TP = 0;FP = 0;TN = 0;FN = 0\n",
        "    # count TP,TN,FP,FN in validation set\n",
        "    '''for i in range(n):\n",
        "      if  self.y_test[i]==1 and y_predict[i]==1:\n",
        "        TP = TP+1\n",
        "      elif self.y_test[i]==0 and y_predict[i]==0:\n",
        "        TN = TN+1\n",
        "      elif self.y_test[i]==0 and y_predict[i]==1:\n",
        "        FP = FP+1\n",
        "      elif self.y_test[i]==1 and y_predict[i]==0:\n",
        "        FN = FN+1'''\n",
        "    for i in range(n):\n",
        "      if  y_test[i]==1 and y_predict[i]==1:\n",
        "        TP = TP+1\n",
        "      elif y_test[i]==0 and y_predict[i]==0:\n",
        "        TN = TN+1\n",
        "      elif y_test[i]==0 and y_predict[i]==1:\n",
        "        FP = FP+1\n",
        "      elif y_test[i]==1 and y_predict[i]==0:\n",
        "        FN = FN+1    \n",
        "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "    # precision = TP/(TP+FP)\n",
        "    # recall = TP/(TP+FN)\n",
        "    # F = 2*precision*recall/(precision+recall)\n",
        "    # specificity = TN/(FP+TN)\n",
        "    # FPR = FP/(FP+TN)\n",
        "    print(\"accuracy:\",accuracy)\n",
        "    # print(\"precision:\",precision)\n",
        "    # print(\"recall:\",recall)\n",
        "    # print(\"F:\",F)\n",
        "    # print(\"specificity:\",specificity)\n",
        "    # print(\"False Positive Rate:\",FPR)\n",
        "    print(\"\")\n",
        "    return accuracy\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ZNHiUUXTq7",
        "outputId": "b10544f1-1335-4e2f-dd92-926c1cd7574a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# figure out which feature is of the most importance\r\n",
        "model = Logistic_regression()\r\n",
        "np.random.shuffle(NorDataset)\r\n",
        "X = NorDataset[:, :-1]  # features\r\n",
        "y = NorDataset[:, -1]   # labels\r\n",
        "w = model.fit(X,y,learning_rate=0.001)\r\n",
        "print(w)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  2.44739104]\n",
            " [  0.88561015]\n",
            " [ -0.19250647]\n",
            " [  2.9123171 ]\n",
            " [  4.89809744]\n",
            " [-16.30535573]\n",
            " [ -3.98992869]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qAaqa2lsZhe"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhmGiMSZ7wqx"
      },
      "source": [
        "class Cross_validation():\n",
        "  def __init__(self, k):\n",
        "    # k: k-fold\n",
        "    self.k = k\n",
        "\n",
        "  def prepare_data(self, data):\n",
        "    # data: np array converted from csv\n",
        "    np.random.shuffle(data)\n",
        "    X = data[:, :-1]  # features\n",
        "    y = data[:, -1]   # labels\n",
        "\n",
        "    # split data into k equal segments, assign them to train and test later\n",
        "    Xs = np.array_split(X, self.k, axis=0)\n",
        "    ys = np.array_split(y, self.k, axis=0)\n",
        "    return Xs, ys\n",
        "\n",
        "  def get_accuracy(self, Xs, ys, lr):\n",
        "    accu_trains = []\n",
        "    accu_tests = []\n",
        "    for i in range(self.k):\n",
        "      X_cv = Xs[:] # X_cross_validation\n",
        "      y_cv = ys[:] # y_cross_validation\n",
        "\n",
        "      X_test = X_cv.pop(i)\n",
        "      y_test = y_cv.pop(i)\n",
        "\n",
        "      X_train = np.concatenate(X_cv)\n",
        "      y_train = np.concatenate(y_cv)\n",
        "\n",
        "      model = Logistic_regression()\n",
        "      w = model.fit(X_train, y_train, lr)\n",
        "\n",
        "      print(\"----------FOLD \", i+1, \"----------\")\n",
        "\n",
        "      print(\"----Train----\")\n",
        "      y_predict_train = model.predict(w, X_train)\n",
        "      accu_train = model.Accu_eval(y_train, y_predict_train)\n",
        "      accu_trains.append(accu_train)\n",
        "\n",
        "      print(\"----Validation----\")\n",
        "      y_predict_test = model.predict(w, X_test)\n",
        "      accu_test = model.Accu_eval(y_test, y_predict_test)\n",
        "      accu_tests.append(accu_test)\n",
        "\n",
        "    return np.mean(accu_trains), np.mean(accu_tests)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8uhC3hsReE"
      },
      "source": [
        "## Experiment with different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx0mwWWwXuMs"
      },
      "source": [
        "lrs = np.logspace(-5, -1, 10) # different learning rates to try\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDataset)\n",
        "for lr in lrs:\n",
        "  print(\"---------------LEARNING RATE: \", lr, \"---------------\")\n",
        "  accu_train_avg, accu_val_avg = cv.get_accuracy(Xs, ys, lr)\n",
        "  print(\"---------------TRAIN AVERAGE ACCURACY\", accu_train_avg, \"---------------\")\n",
        "  print(\"---------------VALIDATION AVERAGE ACCURACY\", accu_val_avg, \"---------------\")\n",
        "  print(\"\\n-------------------------------------------------------------------------------\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTvS3gB7C-_Y"
      },
      "source": [
        "## Experiment with different features\n",
        "\n",
        "During the experiment on different learning rates, we found that the best learning rate is **0.0129**, so we use this learning rate for our experiment with different feature selections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPPRiALdIj2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0519c261-f41a-4e94-b77c-e531186d75e7"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDataset)\n",
        "print(\"----------Using normalized features, without new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY\", accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------Using normalized features, without new features----------\n",
            "----------FOLD  1 ----------\n",
            "----Train----\n",
            "accuracy: 0.8279569892473119\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------FOLD  2 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  3 ----------\n",
            "----Train----\n",
            "accuracy: 0.6487455197132617\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.6451612903225806\n",
            "\n",
            "----------FOLD  4 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  5 ----------\n",
            "----Train----\n",
            "accuracy: 0.8458781362007168\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  6 ----------\n",
            "----Train----\n",
            "accuracy: 0.7813620071684588\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  7 ----------\n",
            "----Train----\n",
            "accuracy: 0.8100358422939068\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  8 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7741935483870968\n",
            "\n",
            "----------FOLD  9 ----------\n",
            "----Train----\n",
            "accuracy: 0.8207885304659498\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  10 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------AVERAGE ACCURACY 0.8387096774193548 ----------\n",
            "\n",
            "---------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8SIJKiY5YX"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(original_data)\n",
        "print(\"----------Using normalized features, with new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY\", accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5HstgXSJ8rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca7fd55-6355-45c0-eb1a-7b933255d7af"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDatasetNew)\n",
        "print(\"----------Using normalized features, with new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY\", accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------Using normalized features, with new features----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------FOLD  1 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7419354838709677\n",
            "\n",
            "----------FOLD  2 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  3 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  4 ----------\n",
            "----Train----\n",
            "accuracy: 0.8494623655913979\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.6774193548387096\n",
            "\n",
            "----------FOLD  5 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  6 ----------\n",
            "----Train----\n",
            "accuracy: 0.8243727598566308\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  7 ----------\n",
            "----Train----\n",
            "accuracy: 0.8207885304659498\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7741935483870968\n",
            "\n",
            "----------FOLD  8 ----------\n",
            "----Train----\n",
            "accuracy: 0.8422939068100358\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8064516129032258\n",
            "\n",
            "----------FOLD  9 ----------\n",
            "----Train----\n",
            "accuracy: 0.7526881720430108\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.6451612903225806\n",
            "\n",
            "----------FOLD  10 ----------\n",
            "----Train----\n",
            "accuracy: 0.8172043010752689\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------AVERAGE ACCURACY 0.7935483870967742 ----------\n",
            "\n",
            "---------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jSYRgbudHMP"
      },
      "source": [
        "## Measure the run time\n",
        "See whether the model converges faster on normalized data than on original data. This is measured by training the model and time it. This part stands on its own, and is not related to any of the above process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGEPixJmmcc"
      },
      "source": [
        "This model class is almost the same as the one before. The only difference is that this model **does not have early stopping**, which can interfere with our timing. Also, this model is only for timing the training process, so it only has a 'fit' method\n",
        "\n",
        "**Should we run this? Should we have early stopping or not?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_eJLxHmX08"
      },
      "source": [
        "class Logistic_regression():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    \n",
        "  # training\n",
        "  def fit(self,X_train,y_train,learning_rate):\n",
        "    #n,m = np.shape(self.X_train)\n",
        "    n,m = np.shape(X_train)  \n",
        "    wk = np.ones([m+1,1]) # wk weights, initialized with 1\n",
        "    wk1 = np.zeros([m+1,1])# wk+1 weights,initialized with 0          \n",
        "    itrnum = 500       # max number of iterations \n",
        "    e = 0.001\n",
        "    der = 0\n",
        "    for k in range(0,itrnum):\n",
        "      for i in range(0,n):\n",
        "        #xi = self.X_train[i].T\n",
        "        xi = X_train[i].T\n",
        "        x0 = np.array([1])\n",
        "        xi = np.concatenate((xi, x0),axis = 0)\n",
        "        #yi = self.y_train[i]\n",
        "        yi = y_train[i]\n",
        "        der = der-xi*(yi-sigmoid(np.matmul(wk[:,0].T,xi))) # take derivative w.r.t w\n",
        "      #wk1[:,0] = wk[:,0]-self.learning_rate*der       # update rule\n",
        "      wk1[:,0] = wk[:,0]-learning_rate*der       # update rule\n",
        "      # if (np.linalg.norm(wk1[:,0]-wk[:,0]))**2<e:         \n",
        "      #   break \n",
        "      # else:\n",
        "      wk = wk1.copy()\n",
        "    return wk1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhONMhAxkuC7"
      },
      "source": [
        "###Using the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T-jHRpsdgHZ"
      },
      "source": [
        "model = Logistic_regression()\n",
        "np.random.shuffle(original_data)\n",
        "X = original_data[:, :-1]  # features\n",
        "y = original_data[:, -1]   # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKriES7le0Pz",
        "outputId": "907be69d-25b3-48da-ce8c-1b9a305a0815"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.00001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.02 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ39ltUkkZJj",
        "outputId": "ff9c8508-e4ff-445b-8e32-1896e5d93709"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.08 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZ_UQERkn5U",
        "outputId": "48a42804-ce91-4ada-b016-2238754a218b"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.11 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aCwwb-clC5m"
      },
      "source": [
        "###Using normalized features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyOcmreeith4"
      },
      "source": [
        "model = Logistic_regression()\n",
        "np.random.shuffle(NorDataset)\n",
        "X = NorDataset[:, :-1]  # features\n",
        "y = NorDataset[:, -1]   # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrPg4E1ti1iV",
        "outputId": "bf200d44-525f-49d9-db09-2edd0f47dd18"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.00001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 3.57 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oePKiDuLlyNs",
        "outputId": "ddb04c20-2c76-430c-d39d-c06392fcf5d4"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 1.84 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44s_vMSPlyNs",
        "outputId": "1a797bf5-0a72-4a28-d0b1-2c4b00229540"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 1.79 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}