{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECSE01_2_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/Try-colabing-in-colab/blob/main/orthopedic_patients_new_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0NkwhApnnJk"
      },
      "source": [
        "# ECSE 551 Mini-project 1\n",
        "*Group 10: Junhao Wang, Yinan Zhou, and Ruilin Ji*\n",
        "\n",
        "This notebook is dedicated for the **Orthopedic patients dataset**, including the model, cross validation, and various experiment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3nM6IIOBf0A"
      },
      "source": [
        "## Start here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oskj54Fv8eGB",
        "outputId": "e3582e5c-7eb0-4f52-c35a-218c8555d53a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrLPT4OTIt5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c53a757-6f83-40ac-91d7-ceadff1842f4"
      },
      "source": [
        "%cd \"/content/drive/My Drive/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFoqVsWOP14"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import scipy.stats\r\n",
        "import statistics"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1gREIMPzw7M"
      },
      "source": [
        "## Orthopedic Patients Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-nyMGN0pKx"
      },
      "source": [
        "# generate new feature by multiplication and normalize\r\n",
        "def newfeature(x,y):\r\n",
        "  z=x*y\r\n",
        "  norz=scipy.stats.zscore(z, axis=0, ddof=0, nan_policy='propagate')\r\n",
        "  return norz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc7bhCtyW17b"
      },
      "source": [
        "# convert csv to dataframe\n",
        "df = pd.read_csv('orthopedic_patients.csv')\n",
        "original_data = df.to_numpy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybt1JT3h0JpU"
      },
      "source": [
        "# normalize feature\r\n",
        "NorData = scipy.stats.zscore(df.iloc[:,:-1], axis=0, ddof=0, nan_policy='propagate')\r\n",
        "# normalized dataset\r\n",
        "NorDataset = np.column_stack((NorData, df['Class']))\r\n",
        "# np.savetxt('normalized_orthopedic_patients.csv', NorPatientData, delimiter=',')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oDHjN7u0rbT"
      },
      "source": [
        "New feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drHfVd9v2elN"
      },
      "source": [
        "# new feature\r\n",
        "NewF1 = newfeature(df.pelvic_incidence, df.sacral_slope) # new feature by multiplying two high-correlation feature\r\n",
        "NewF2 = df.iloc[:,0]             # initialize new feature 2 using 1st feature\r\n",
        "n_row = np.shape(original_data)[0]\r\n",
        "NewF = np.zeros(n_row)            # initialize new features using 0s\r\n",
        "for col in range(6):\r\n",
        "  # new feature 2: multiplying all features\r\n",
        "  if col>0:\r\n",
        "    NewF2 = newfeature(NewF2,df.iloc[:,col])\r\n",
        "  # new feature 3-8: square all columns\r\n",
        "  new = newfeature(df.iloc[:,col],df.iloc[:,col]) # square feature\r\n",
        "  NewF = np.column_stack((NewF,new))\r\n",
        "\r\n",
        "NewF = np.delete(NewF,0,1)\r\n",
        "# print(NewF2)\r\n",
        "# print(np.shape(NewF))\r\n",
        "# normalized dataset with new feature\r\n",
        "NorDatasetNew = np.column_stack((NorData,NewF1,NewF2,NewF,df['Class']))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwnmcLuQsdWb"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYrl86Dhy9jy"
      },
      "source": [
        "# sigmoid function\n",
        "def sigmoid(a):\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "class Logistic_regression():\n",
        "  def __init__(self):#,X_train,y_train,learning_rate,X_test,y_test):\n",
        "    pass\n",
        "    \n",
        "  # training\n",
        "  def fit(self,X_train,y_train,learning_rate):\n",
        "    #n,m = np.shape(self.X_train)\n",
        "    n,m = np.shape(X_train)  \n",
        "    wk = np.ones([m+1,1]) # wk weights, initialized with 1\n",
        "    wk1 = np.zeros([m+1,1])# wk+1 weights,initialized with 0          \n",
        "    itrnum = 500       # max number of iterations \n",
        "    e = 0.001\n",
        "    der = 0\n",
        "    for k in range(0,itrnum):\n",
        "      for i in range(0,n):\n",
        "        #xi = self.X_train[i].T\n",
        "        xi = X_train[i].T\n",
        "        x0 = np.array([1])\n",
        "        xi = np.concatenate((xi, x0),axis = 0)\n",
        "        #yi = self.y_train[i]\n",
        "        yi = y_train[i]\n",
        "        der = der-xi*(yi-sigmoid(np.matmul(wk[:,0].T,xi))) # take derivative w.r.t w\n",
        "      #wk1[:,0] = wk[:,0]-self.learning_rate*der       # update rule\n",
        "      wk1[:,0] = wk[:,0]-learning_rate*der       # update rule\n",
        "      if (np.linalg.norm(wk1[:,0]-wk[:,0]))**2<e:         \n",
        "        break \n",
        "      else:\n",
        "        wk = wk1.copy()\n",
        "    return wk1\n",
        "  \n",
        "  # validation\n",
        "  def predict(self,w,X_test):\n",
        "    #n,m = np.shape(self.X_test)\n",
        "    n,m = np.shape(X_test)   \n",
        "    y_predict = np.zeros([n,1])\n",
        "    for i in range(0,n):\n",
        "      #xi = self.X_test[i].T\n",
        "      xi = X_test[i].T\n",
        "      x0 = np.array([1])\n",
        "      xi = np.concatenate((xi, x0),axis = 0)\n",
        "      p1 = sigmoid(np.matmul(w.T,xi)) # calculate probabilities p(y=1|x)\n",
        "      # covert probabilities to 0 or 1 by thresholding at 0.5\n",
        "      if p1>=0.5:\n",
        "        y_predict[i] = 1\n",
        "      else:\n",
        "        y_predict[i] = 0\n",
        "    return y_predict\n",
        "\n",
        "  # evaluate accuracy\n",
        "  def Accu_eval(self,y_test,y_predict):\n",
        "    #y_predict = self.predict(X_test)\n",
        "    n,j = np.shape(y_predict)\n",
        "    TP = 0;FP = 0;TN = 0;FN = 0\n",
        "    # count TP,TN,FP,FN in validation set\n",
        "    '''for i in range(n):\n",
        "      if  self.y_test[i]==1 and y_predict[i]==1:\n",
        "        TP = TP+1\n",
        "      elif self.y_test[i]==0 and y_predict[i]==0:\n",
        "        TN = TN+1\n",
        "      elif self.y_test[i]==0 and y_predict[i]==1:\n",
        "        FP = FP+1\n",
        "      elif self.y_test[i]==1 and y_predict[i]==0:\n",
        "        FN = FN+1'''\n",
        "    for i in range(n):\n",
        "      if  y_test[i]==1 and y_predict[i]==1:\n",
        "        TP = TP+1\n",
        "      elif y_test[i]==0 and y_predict[i]==0:\n",
        "        TN = TN+1\n",
        "      elif y_test[i]==0 and y_predict[i]==1:\n",
        "        FP = FP+1\n",
        "      elif y_test[i]==1 and y_predict[i]==0:\n",
        "        FN = FN+1    \n",
        "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "    # precision = TP/(TP+FP)\n",
        "    # recall = TP/(TP+FN)\n",
        "    # F = 2*precision*recall/(precision+recall)\n",
        "    # specificity = TN/(FP+TN)\n",
        "    # FPR = FP/(FP+TN)\n",
        "    print(\"accuracy:\",accuracy)\n",
        "    # print(\"precision:\",precision)\n",
        "    # print(\"recall:\",recall)\n",
        "    # print(\"F:\",F)\n",
        "    # print(\"specificity:\",specificity)\n",
        "    # print(\"False Positive Rate:\",FPR)\n",
        "    print(\"\")\n",
        "    return accuracy\n",
        "    "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ZNHiUUXTq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d558a5f2-bf6e-4000-f172-d0ba747523cd"
      },
      "source": [
        "# figure out which feature is of the most importance\r\n",
        "model = Logistic_regression()\r\n",
        "np.random.shuffle(NorDataset)\r\n",
        "X = NorDataset[:, :-1]  # features\r\n",
        "y = NorDataset[:, -1]   # labels\r\n",
        "w = model.fit(X,y,learning_rate=0.001)\r\n",
        "print(w)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[  2.44739104]\n",
            " [  0.88561015]\n",
            " [ -0.19250647]\n",
            " [  2.9123171 ]\n",
            " [  4.89809744]\n",
            " [-16.30535573]\n",
            " [ -3.98992869]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qAaqa2lsZhe"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhmGiMSZ7wqx"
      },
      "source": [
        "class Cross_validation():\n",
        "  def __init__(self, k):\n",
        "    # k: k-fold\n",
        "    self.k = k\n",
        "\n",
        "  def prepare_data(self, data):\n",
        "    # data: np array converted from csv\n",
        "    np.random.shuffle(data)\n",
        "    X = data[:, :-1]  # features\n",
        "    y = data[:, -1]   # labels\n",
        "\n",
        "    # split data into k equal segments, assign them to train and test later\n",
        "    Xs = np.array_split(X, self.k, axis=0)\n",
        "    ys = np.array_split(y, self.k, axis=0)\n",
        "    return Xs, ys\n",
        "\n",
        "  def get_accuracy(self, Xs, ys, lr):\n",
        "    accu_trains = []\n",
        "    accu_tests = []\n",
        "    for i in range(self.k):\n",
        "      X_cv = Xs[:] # X_cross_validation\n",
        "      y_cv = ys[:] # y_cross_validation\n",
        "\n",
        "      X_test = X_cv.pop(i)\n",
        "      y_test = y_cv.pop(i)\n",
        "\n",
        "      X_train = np.concatenate(X_cv)\n",
        "      y_train = np.concatenate(y_cv)\n",
        "\n",
        "      model = Logistic_regression()\n",
        "      w = model.fit(X_train, y_train, lr)\n",
        "\n",
        "      print(\"----------FOLD \", i+1, \"----------\")\n",
        "\n",
        "      print(\"----Train----\")\n",
        "      y_predict_train = model.predict(w, X_train)\n",
        "      accu_train = model.Accu_eval(y_train, y_predict_train)\n",
        "      accu_trains.append(accu_train)\n",
        "\n",
        "      print(\"----Validation----\")\n",
        "      y_predict_test = model.predict(w, X_test)\n",
        "      accu_test = model.Accu_eval(y_test, y_predict_test)\n",
        "      accu_tests.append(accu_test)\n",
        "\n",
        "    return np.mean(accu_trains), np.mean(accu_tests)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8uhC3hsReE"
      },
      "source": [
        "## Experiment with different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx0mwWWwXuMs"
      },
      "source": [
        "lrs = np.logspace(-5, -1, 10) # different learning rates to try\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDataset)\n",
        "for lr in lrs:\n",
        "  print(\"---------------LEARNING RATE: \", lr, \"---------------\")\n",
        "  accu_train_avg, accu_val_avg = cv.get_accuracy(Xs, ys, lr)\n",
        "  print(\"---------------TRAIN AVERAGE ACCURACY\", accu_train_avg, \"---------------\")\n",
        "  print(\"---------------VALIDATION AVERAGE ACCURACY\", accu_val_avg, \"---------------\")\n",
        "  print(\"\\n-------------------------------------------------------------------------------\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTvS3gB7C-_Y"
      },
      "source": [
        "## Experiment with different features\n",
        "\n",
        "During the experiment on different learning rates, we found that the best learning rate is **0.0129**, so we use this learning rate for our experiment with different feature selections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPPRiALdIj2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1e2f18-3c4e-4383-9b2f-965a83bb79ba"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDataset)\n",
        "print(\"----------Using normalized features, without new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY\", accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------Using normalized features, without new features----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------FOLD  1 ----------\n",
            "----Train----\n",
            "accuracy: 0.8494623655913979\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8064516129032258\n",
            "\n",
            "----------FOLD  2 ----------\n",
            "----Train----\n",
            "accuracy: 0.8207885304659498\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7741935483870968\n",
            "\n",
            "----------FOLD  3 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  4 ----------\n",
            "----Train----\n",
            "accuracy: 0.8136200716845878\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7741935483870968\n",
            "\n",
            "----------FOLD  5 ----------\n",
            "----Train----\n",
            "accuracy: 0.8602150537634409\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.7741935483870968\n",
            "\n",
            "----------FOLD  6 ----------\n",
            "----Train----\n",
            "accuracy: 0.8315412186379928\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  7 ----------\n",
            "----Train----\n",
            "accuracy: 0.8458781362007168\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  8 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  9 ----------\n",
            "----Train----\n",
            "accuracy: 0.8279569892473119\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  10 ----------\n",
            "----Train----\n",
            "accuracy: 0.7921146953405018\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.5806451612903226\n",
            "\n",
            "----------AVERAGE ACCURACY 0.8064516129032258 ----------\n",
            "\n",
            "---------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8SIJKiY5YX"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(original_data)\n",
        "print(\"----------Using normalized features, with new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY\", accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5HstgXSJ8rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535cd875-a8bc-4592-92de-0dcc747a174a"
      },
      "source": [
        "lr = 0.0129\n",
        "cv = Cross_validation(10) # 10-fold cross-validation\n",
        "Xs, ys = cv.prepare_data(NorDatasetNew)\n",
        "print(\"----------Using normalized features, with new features----------\")\n",
        "accu_avg_train, accu_avg_val = cv.get_accuracy(Xs, ys, lr)\n",
        "print(\"----------AVERAGE ACCURACY: train-\", accu_avg_train,\" vs. validation-\",accu_avg_val, \"----------\")\n",
        "print(\"\\n---------------------------------------------------------------------\")\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------Using normalized features, with new features----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------FOLD  1 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  2 ----------\n",
            "----Train----\n",
            "accuracy: 0.8566308243727598\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  3 ----------\n",
            "----Train----\n",
            "accuracy: 0.8745519713261649\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.6774193548387096\n",
            "\n",
            "----------FOLD  4 ----------\n",
            "----Train----\n",
            "accuracy: 0.8422939068100358\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  5 ----------\n",
            "----Train----\n",
            "accuracy: 0.8637992831541219\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  6 ----------\n",
            "----Train----\n",
            "accuracy: 0.8602150537634409\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8064516129032258\n",
            "\n",
            "----------FOLD  7 ----------\n",
            "----Train----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.9032258064516129\n",
            "\n",
            "----------FOLD  8 ----------\n",
            "----Train----\n",
            "accuracy: 0.8566308243727598\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8709677419354839\n",
            "\n",
            "----------FOLD  9 ----------\n",
            "----Train----\n",
            "accuracy: 0.8637992831541219\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------FOLD  10 ----------\n",
            "----Train----\n",
            "accuracy: 0.8817204301075269\n",
            "\n",
            "----Validation----\n",
            "accuracy: 0.8387096774193549\n",
            "\n",
            "----------AVERAGE ACCURACY: train- 0.8577060931899642  vs. validation- 0.8419354838709676 ----------\n",
            "\n",
            "---------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jSYRgbudHMP"
      },
      "source": [
        "## Measure the run time\n",
        "See whether the model converges faster on normalized data than on original data. This is measured by training the model and time it. This part stands on its own, and is not related to any of the above process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGEPixJmmcc"
      },
      "source": [
        "This model class is almost the same as the one before. The only difference is that this model **does not have early stopping**, which can interfere with our timing. Also, this model is only for timing the training process, so it only has a 'fit' method\n",
        "\n",
        "**Should we run this? Should we have early stopping or not?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_eJLxHmX08"
      },
      "source": [
        "class Logistic_regression():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    \n",
        "  # training\n",
        "  def fit(self,X_train,y_train,learning_rate):\n",
        "    #n,m = np.shape(self.X_train)\n",
        "    n,m = np.shape(X_train)  \n",
        "    wk = np.ones([m+1,1]) # wk weights, initialized with 1\n",
        "    wk1 = np.zeros([m+1,1])# wk+1 weights,initialized with 0          \n",
        "    itrnum = 500       # max number of iterations \n",
        "    e = 0.001\n",
        "    der = 0\n",
        "    for k in range(0,itrnum):\n",
        "      for i in range(0,n):\n",
        "        #xi = self.X_train[i].T\n",
        "        xi = X_train[i].T\n",
        "        x0 = np.array([1])\n",
        "        xi = np.concatenate((xi, x0),axis = 0)\n",
        "        #yi = self.y_train[i]\n",
        "        yi = y_train[i]\n",
        "        der = der-xi*(yi-sigmoid(np.matmul(wk[:,0].T,xi))) # take derivative w.r.t w\n",
        "      #wk1[:,0] = wk[:,0]-self.learning_rate*der       # update rule\n",
        "      wk1[:,0] = wk[:,0]-learning_rate*der       # update rule\n",
        "      # if (np.linalg.norm(wk1[:,0]-wk[:,0]))**2<e:         \n",
        "      #   break \n",
        "      # else:\n",
        "      wk = wk1.copy()\n",
        "    return wk1"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhONMhAxkuC7"
      },
      "source": [
        "###Using the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T-jHRpsdgHZ"
      },
      "source": [
        "model = Logistic_regression()\n",
        "np.random.shuffle(original_data)\n",
        "X = original_data[:, :-1]  # features\n",
        "y = original_data[:, -1]   # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKriES7le0Pz",
        "outputId": "907be69d-25b3-48da-ce8c-1b9a305a0815"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.00001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.02 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ39ltUkkZJj",
        "outputId": "ff9c8508-e4ff-445b-8e32-1896e5d93709"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.08 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZ_UQERkn5U",
        "outputId": "48a42804-ce91-4ada-b016-2238754a218b"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 2.11 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aCwwb-clC5m"
      },
      "source": [
        "###Using normalized features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyOcmreeith4"
      },
      "source": [
        "model = Logistic_regression()\n",
        "np.random.shuffle(NorDataset)\n",
        "X = NorDataset[:, :-1]  # features\n",
        "y = NorDataset[:, -1]   # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrPg4E1ti1iV",
        "outputId": "bf200d44-525f-49d9-db09-2edd0f47dd18"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.00001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 3.57 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oePKiDuLlyNs",
        "outputId": "ddb04c20-2c76-430c-d39d-c06392fcf5d4"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 1.84 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44s_vMSPlyNs",
        "outputId": "1a797bf5-0a72-4a28-d0b1-2c4b00229540"
      },
      "source": [
        "%%timeit\n",
        "w = model.fit(X, y, learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 1.79 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}